import sys
import torch
import logging
from torch.distributions import Categorical
from utils import get_device, get_log_probs_precision, get_scalars, set_scalars
from dist import NegativeBinomial

LOG_PROBS_PRECISION = get_log_probs_precision()

logger = logging.getLogger(__name__)


class CategoricalEmission(torch.nn.Module):
    """
    Categoical emission from a bookend + n_states hidden states, (0, 1, .., n_states),
    to n_obvs emission classes.
    """

    def __init__(self, n_states, n_obvs, diag=False, device=None):
        super(CategoricalEmission, self).__init__()

        self.device = get_device() if device is None else device

        # NB we assume a bookend state to transition in/out.
        self.n_states = 1 + n_states

        # NB number of possible observable classes + hidden observation emitted by bookend state.
        self.n_obvs = 1 + n_obvs

        logger.info(
            f"Creating CategoricalEmission (diag={diag}) with {n_states} hidden states & {n_obvs} observed classes on device={self.device}"
        )

        self.log_em = self.init_emission(diag=diag)

        # NB emissions to/from bookend state should not be trained.
        self.log_em_grad_mask = torch.ones(
            (self.n_states, self.n_obvs), requires_grad=False, device=self.device
        )

        self.log_em_grad_mask[0, :] = 0
        self.log_em_grad_mask[:, 0] = 0

    def init_emission(self, log_probs_precision=LOG_PROBS_PRECISION, diag=False):
        # NB simple Markov model, where the hidden state is emitted.
        if diag:
            log_em = (
                torch.eye(self.n_states, self.n_obvs, device=self.device, requires_grad=True)
                .log()
                .clip(min=log_probs_precision, max=-log_probs_precision)
            )
        else:
            log_em = torch.randn(self.n_states, self.n_obvs, device=self.device, requires_grad=True)

        # NB nn.Parameter marks this to be optimised via torch.
        log_em = torch.nn.Parameter(log_em)
        log_em.data = CategoricalEmission.normalize_emission(
            log_em.data, log_probs_precision=log_probs_precision
        )

        return log_em

    @classmethod
    def normalize_emission(cls, log_em, log_probs_precision=LOG_PROBS_PRECISION):
        # NB emit only a bookend token from the bookend state.
        log_em[0, :] = log_probs_precision
        log_em[0, 0] = 0.0

        # NB only the bookend state emits a bookend token, to machine precision.
        log_em[1:, 0] = log_probs_precision

        # NB see https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html
        return log_em.log_softmax(dim=1)

    def log_emission(self, state, obs):
        """
        Getter for log_em with broadcasting.
        """
        if state is None:
            return self.log_em[:, obs]
        elif obs is None:
            return self.log_em[state, :]
        else:
            return set_scalars(self.log_em[state, obs], device=self.device)

    def sample(self, state):
        probs = [self.log_emission(ss, None).exp() for ss in get_scalars(state)]
        result = [Categorical(pp).sample() for pp in probs]        
        return torch.tensor(result, dtype=torch.int32, device=self.device)

    def mask_grad(self):
        self.log_em.grad *= self.log_em_grad_mask
        return self
    
    def forward(self, obs):
        # NB equivalent to normalized self.emission(None, obs) bar bookend row.
        return self.log_emission(None, obs).log_softmax(dim=0)
        
    def finalize_training(self):
        self.log_em.data = CategoricalEmission.normalize_emission(
            self.log_em.data
        )
        
        return self
        
    def validate(self):
        logger.info(f"Emission log probability matrix:\n{self.log_em}\n")

    def to_device(self, device):
        self.device = device
        self.log_em = self.log_em.to(device)
        self.log_em_grad_mask = self.log_em_grad_mask.to(device)

        return self

    @property
    def parameters_dict(self):
        """
        Dict with named torch parameters.
        """
        return {"log_em": self.log_em}

class BookendDist:
    def __init__(self, device=None):
        self.device = get_device() if device is None else get_device()
        
    def sample(self):
        return set_scalars(0, device=self.device)

    def log_prob(self, obs):
        if obs.dim() > 0:
            result = torch.zeros(len(obs), dtype=torch.int32, device=self.device)
            result[obs > 0] = LOG_PROBS_PRECISION
            return result
        else:
            return set_scalars(0, device=self.device)
